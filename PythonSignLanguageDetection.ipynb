{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3cce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Originally based off of the 'Sign Language Detection Using Action Recognition' Python tutorial by Nicholas Renotte\n",
    "Tutorial Video Link: 'https://www.youtube.com/watch?v=doDUihpj6ro'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs Dependencies\n",
    "\n",
    "# Allows for the easier visualization of images\n",
    "#!pip3 install matplotlib\n",
    "\n",
    "# Enables for the extraction of keypoints from images\n",
    "#!pip3 install mediapipe\n",
    "\n",
    "# Computer vision library which can work with computer webcams\n",
    "#!pip3 install opencv-python\n",
    "\n",
    "# Primarily for dataset splitting in this scenario\n",
    "#!pip3 install sklearn\n",
    "\n",
    "#!pip3 install tensorflow\n",
    "#!pip3 install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Dependencies\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58213883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keypoints Using Media Pipe Holistic\n",
    "\n",
    "# Brings in a pre-existing mediapipe holisitc model that actually carries out detection\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Drawing utilities used by the mediapipe holisitc model to draw out detections from an image back onto it\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdddde8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in modelized landmark data and draws it back onto an image\n",
    "def draw_landmarks(image, results):\n",
    "    \n",
    "    # Draws facial connections\n",
    "    mp_drawing.draw_landmarks(image, \n",
    "                              results.face_landmarks , \n",
    "                              mp_holistic.FACE_CONNECTIONS, \n",
    "                              \n",
    "                              # Joints\n",
    "                              mp_drawing.DrawingSpec(color = (80, 110, 10), \n",
    "                                                     thickness = 1, \n",
    "                                                     circle_radius = 1), \n",
    "                              \n",
    "                              # Lines\n",
    "                              mp_drawing.DrawingSpec(color = (80, 256, 121), \n",
    "                                                     thickness = 1, \n",
    "                                                     circle_radius = 1))\n",
    "    \n",
    "    # Draws overall body pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks , mp_holistic.POSE_CONNECTIONS)\n",
    "    \n",
    "    # Draws left hand connection\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks , mp_holistic.HAND_CONNECTIONS)\n",
    "    \n",
    "    # Draws right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks , mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzes an image and returns modelized landmarks of said image\n",
    "def mediapipe_detection(image, model):\n",
    "    \n",
    "    # Converts the image\n",
    "    \n",
    "    # Color conversion from Blue, Green, Red to Red, Green, Blue\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Saves on memory by no longer making the image writeable\n",
    "    image.flags.writeable = False\n",
    "    \n",
    "    # Carries out a prediction upon the image with the given pre-existing model\n",
    "    results = model.process(image)\n",
    "    \n",
    "    # Unconverts the image\n",
    "    \n",
    "    # Renders the image writeable once more\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    # Reconverts the image back to its original color scheme\n",
    "    image.cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Returns the image along with the given model predictions\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Has CV2 continously take images via the given camera and looping through them, therefore appearing\n",
    "    as if presenting live video, before halting the process upon hitting the 'q' key upon the keyboard\n",
    "'''\n",
    "image_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Sets the mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, \n",
    "                          min_tracking_confidence = 0.5) as holistic:\n",
    "    \n",
    "    while image_capture.isOpened():\n",
    "\n",
    "        # Reeds in the frame from the given camera\n",
    "        result, frame = image_capture.read()\n",
    "\n",
    "        # Carries out keypoint detection upon the current frame\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "            \n",
    "        # Draws keypoint landmarks\n",
    "        draw_landmarks(image, results)\n",
    "\n",
    "        # Dispalys the current frame to the user via a GUI window\n",
    "        cv2.imshow('OpenCV Video Camera Feed', image)\n",
    "\n",
    "        # Breaks out of the loop gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "image_capture.realease()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts Keypoint Values\n",
    "\n",
    "'''\n",
    "Each landmark is an array of coordinates, varying in terms of how made types depending upon the sort\n",
    "    of landmark, so this and the fact that the model may fail to detect them for whatever reason, such\n",
    "    as a hand out of frame, must be taken into consideration when recording these values, along with\n",
    "    the fact that they must be flattened into a single 1 dimensional array for their purpose as input\n",
    "    into a neural network later on\n",
    "Essentially concatenates all keypoints into a single, flat numpy array for sign language detection learning, \n",
    "    the neural network decoding these values to learn what is what\n",
    "'''\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[result.x, result.y, result.z, result.visibility] for result in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4)\n",
    "    face = np.array([[result.x, result.y, result.z] for result in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468 * 3)\n",
    "    left_hand = np.array([[result.x, result.y, result.z] for result in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    right_hand = np.array([[result.x, result.y, result.z] for result in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    \n",
    "    return np.concatenate([pose, face, left_hand, right_hand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(468 * 3) + (33 * 4) + (21 * 3) + (21 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aaea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets Up Folders For Collection\n",
    "\n",
    "# File and directory path for exported numpy array data\n",
    "DATA_PATH = os.path.join('../../Data/SignLanguageRecognition')\n",
    "\n",
    "# Actions to be detected\n",
    "actions = np.array(['hello', 'thanks', 'i_love_you'])\n",
    "\n",
    "'''\n",
    "30 frames of sequential keypoints used in order to attempt to classify an action \n",
    "    via genuine action detection rather than through the use of a single frame\n",
    "    in terms of computer vision\n",
    "'''\n",
    "\n",
    "# 30 Videos worth of data\n",
    "number_of_sequences = 30\n",
    "\n",
    "# Each video is 30 frames in length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0008c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves each of the images extracted keypoint data as a file in action delemited folders\n",
    "for action in actions:\n",
    "    for sequence in range(number_of_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects Keypoint Values For Training & Testing\n",
    "\n",
    "image_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Sets the mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, \n",
    "                          min_tracking_confidence = 0.5) as holistic:\n",
    "    \n",
    "    # Loops through the available actions\n",
    "    for action in actions:\n",
    "        \n",
    "        # Loops through the available videos or sequences\n",
    "        for sequence in range(number_of_sequences):\n",
    "            \n",
    "            # Loops throug the video's entire length, set as the sequence length or number of frames\n",
    "            for frame_number in range(sequence_length):\n",
    "                \n",
    "                result, frame = image_capture.read()\n",
    "\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                draw_landmarks(image, results)\n",
    "                \n",
    "                '''\n",
    "                Applies collection logic\n",
    "                Collection breaks used between each sequence in order to allow time to reset and \n",
    "                    reposition in order to properly collect each action from stat to finish\n",
    "                '''\n",
    "                if frame_number == 0:\n",
    "                    \n",
    "                    '''\n",
    "                    Variables include the image, text, text postion, font type, font size, front color, \n",
    "                        line thickness and line size\n",
    "                    '''\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120, 200), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                    \n",
    "                    cv2.putText(image, f'Collecting frames for {action} Video Number {sequence}', (15, 12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                    cv2.imshow('OpenCV Video Camera Feed', image)\n",
    "                    \n",
    "                    # For each video a 2 second break will occur upon the first frame of a new video\n",
    "                    cv2.waitKey(2000)\n",
    "                \n",
    "                else:\n",
    "                    cv2.putText(image, f'Collecting frames for {action} Video Number {sequence}', (15, 12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                    cv2.imshow('OpenCV Video Camera Feed', image)\n",
    "\n",
    "                # Exports the new keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                numpy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_number))\n",
    "                np.save(numpy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "    \n",
    "    image_capture.realease()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c185fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preporcesses Data & Creates Labels & Features\n",
    "\n",
    "'''\n",
    "Creates a dictionary where the key is an action and its value is its index number from the\n",
    "    orginal action numpy array, which will then be used to label the saved data for supervised\n",
    "    learning purposes\n",
    "'''\n",
    "label_map = {label:number for number, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58517667",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A single giant array of values containing all of the data is desired\n",
    "End up with 90 arrays of videos, each video with 30 frames, each frame having 1662 values representing\n",
    "    the flatten keypoints for the face, pose and left and right hand coordinates\n",
    "'''\n",
    "\n",
    "sequences, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    for squence in range(number_of_sequences):\n",
    "        window = []\n",
    "        \n",
    "        for frame_number in range(sequence_length):\n",
    "            result = np.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196afd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf98646",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b52bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0792eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training, x_testing, y_training, y_testing = train_test_split(x, y, test_size = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40044a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds & Trains LSTM Neural Network\n",
    "\n",
    "logs_directory = os.path.join('Logs')\n",
    "tensorboard_callback = TensorBoard(log_dir = logs_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b104630",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Many professional neural networks use CNN layers or pre-trained models followed by LSTM layers, \n",
    "    the reasons being that they require less data to produce fairly accurate results, are faster\n",
    "    to train due to being far less dense in terms of the added layers and therefore connections\n",
    "    between the available neurons, and because of the simpler neural network, also faster when\n",
    "    carrying out action detection in real time\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "'''\n",
    "When utilizing LSTM layers, you must return the sequences if the following stacked layer is also an LSTM type\n",
    "The input shape is of 30 image frames per prediction, with each frame having 1662 keypoint values\n",
    "'''\n",
    "model.add(LSTM(64, return_sequences = True, activation = 'relu', input_shape = (30, 1662)))\n",
    "model.add(LSTM(128, return_sequences = True, activation = 'relu'))\n",
    "model.add(LSTM(64, return_sequences = False, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "\n",
    "'''\n",
    "Return output of the model are values within a probability of 0 to 1, \n",
    "    with the sum of all the values added together adding up to 1, taking\n",
    "    the maximum of those values are the final answer\n",
    "    Example output: [0.7, 0.2, 0.1] each value being for each action available\n",
    "'''\n",
    "model.add(Dense(actions.shape[0], activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_result = [0.7, 0.2, 0.1]\n",
    "print(actions[np.argmax(example_result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad508d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For multiclass classification models, you must use the categorical crossentropy loss optimizer\n",
    "For binary classification models, binary crossentropy loss optimize is preferred\n",
    "'''\n",
    "model.compile(optimizer = 'Adam', \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426590ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a01f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_training, y_training, epochs = 200, callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes Predictions\n",
    "\n",
    "y_hat = model.predict(x_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actions[np.argmax(y_hat[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves Model Weights\n",
    "saved_model_weights_folder = '../../Data/Models'\n",
    "model.save(os.path.join(saved_model_weights_folder, 'sign_language_detection.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67555644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(saved_model_weights_folder, 'sign_language_detection.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aea0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates The Model Via Confusion Matrix & Accuracy\n",
    "\n",
    "# Converting results from [1, 0, 0], [0, 1, 0] and [0, 0, 1] to 0, 1 and 2\n",
    "y_true = np.argmax(y_testing, axis = 1).tolist()\n",
    "y_hat = np.argmax(y_hat, axis = 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d131fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_visualization(result, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for number, probability in enumerate(result):\n",
    "        cv2.rectangle(output_frame, \n",
    "                      (0, 60 + number * 40), \n",
    "                      (int(probability * 100), 90 + number * 40), \n",
    "                      colors[number], \n",
    "                      -1)\n",
    "        cv2.putText(output_frame, \n",
    "                    actions[number], \n",
    "                    (0, 85 + number * 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), \n",
    "                    2, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests Model In Real Time\n",
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.4\n",
    "\n",
    "image_capture = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, \n",
    "                          min_tracking_confidence = 0.5) as holistic:\n",
    "    \n",
    "    while image_capture.isOpened():\n",
    "\n",
    "        result, frame = image_capture.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        # Prediction Logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            prediction = model.predict(np.expand_dims(sequence, axis = 0))[0]\n",
    "            print(actions[np.argmax(prediction)])\n",
    "            predictions.append(np.argmax(prediction))\n",
    "\n",
    "        # Visualization Logic\n",
    "        \n",
    "        # Checks the last 10 frames to avoid false detections in a middle of an action\n",
    "        if np.unique(predictions[-10:] == np.argmax(prediction))\n",
    "        \n",
    "            # If maximum prediction is above the threshold\n",
    "            if prediction[np.argmax(prediction)] > threshold:\n",
    "\n",
    "                # Continous detection is ongoing, so only append if a new action is being taken\n",
    "                if len(sentence) > 0:\n",
    "\n",
    "                    # Check that the current action is not the last action, otherwise known as an ongoing action\n",
    "                    if actions[np.argmax(prediction)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmx(res)])\n",
    "                    \n",
    "        if len(sentence) > 5:\n",
    "            sentence = sentence[-5:]\n",
    "        \n",
    "        # Renders the visualization on the screen\n",
    "        \n",
    "        # Variables are starting point, size of box, box color and filling out said box\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ''.join(sentence), (3, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                  \n",
    "        cv2.imshow('OpenCV Video Camera Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "image_capture.realease()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66593f78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
